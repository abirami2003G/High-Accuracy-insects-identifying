from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

import os

# Specify the path to your desired directory
path_to_directory = '/content/drive/MyDrive/core project1'

# Change the working directory to the specified path
os.chdir(path_to_directory)

# Import necessary libraries
import os
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam

# Check if GPU is available and set it as the runtime
if tf.test.gpu_device_name():
    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))
else:
    print("No GPU available. Switch to a GPU runtime in Google Colab.")

# Define the directories for your dataset
train_dir = '/content/drive/MyDrive/core project1'  # Replace with the path to your training dataset
test_dir = '/content/drive/MyDrive/core project1'    # Replace with the path to your test dataset

# Set parameters for data augmentation and preprocessing
train_data_gen = ImageDataGenerator(rescale=1./255, rotation_range=40, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')
test_data_gen = ImageDataGenerator(rescale=1./255)  # Don't apply data augmentation to the test data

# Create generators for loading and augmenting images
batch_size = 32
train_generator = train_data_gen.flow_from_directory(train_dir, target_size=(150, 150), batch_size=batch_size, class_mode='binary')
test_generator = test_data_gen.flow_from_directory(test_dir, target_size=(150, 150), batch_size=batch_size, class_mode='binary')

model = Sequential()

model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))
model.add(MaxPooling2D(2, 2))

model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D(2, 2))

model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D(2, 2))

model.add(Flatten())

model.add(Dense(512, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])

epochs = 20
history = model.fit(train_generator, steps_per_epoch=train_generator.samples // batch_size, epochs=epochs, validation_data=test_generator, validation_steps=test_generator.samples // batch_size)

Epoch 1/20
16/18 [=========================>....] - ETA: 9s - loss: 0.7371 - accuracy: 0.4729 /usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
18/18 [==============================] - 111s 6s/step - loss: 0.7320 - accuracy: 0.4813 - val_loss: 0.6913 - val_accuracy: 0.5347
Epoch 2/20
18/18 [==============================] - 61s 3s/step - loss: 0.6907 - accuracy: 0.5488 - val_loss: 0.6883 - val_accuracy: 0.5382
Epoch 3/20
18/18 [==============================] - 63s 3s/step - loss: 0.6926 - accuracy: 0.5346 - val_loss: 0.6865 - val_accuracy: 0.5729
Epoch 4/20
18/18 [==============================] - 67s 4s/step - loss: 0.6859 - accuracy: 0.5453 - val_loss: 0.6808 - val_accuracy: 0.5868
Epoch 5/20
18/18 [==============================] - 62s 4s/step - loss: 0.6856 - accuracy: 0.5275 - val_loss: 0.6760 - val_accuracy: 0.6042
Epoch 6/20
18/18 [==============================] - 63s 4s/step - loss: 0.6796 - accuracy: 0.5702 - val_loss: 0.6787 - val_accuracy: 0.5747
Epoch 7/20
18/18 [==============================] - 64s 4s/step - loss: 0.6806 - accuracy: 0.5844 - val_loss: 0.6723 - val_accuracy: 0.5694
Epoch 8/20
18/18 [==============================] - 61s 3s/step - loss: 0.6687 - accuracy: 0.6110 - val_loss: 0.6675 - val_accuracy: 0.5972
Epoch 9/20
18/18 [==============================] - 63s 4s/step - loss: 0.6710 - accuracy: 0.6075 - val_loss: 0.6606 - val_accuracy: 0.6215
Epoch 10/20
18/18 [==============================] - 62s 3s/step - loss: 0.6752 - accuracy: 0.5861 - val_loss: 0.6742 - val_accuracy: 0.5781
Epoch 11/20
18/18 [==============================] - 63s 3s/step - loss: 0.6768 - accuracy: 0.5808 - val_loss: 0.6774 - val_accuracy: 0.5486
Epoch 12/20
18/18 [==============================] - 83s 5s/step - loss: 0.6615 - accuracy: 0.6252 - val_loss: 0.6530 - val_accuracy: 0.6024
Epoch 13/20
18/18 [==============================] - 58s 3s/step - loss: 0.6597 - accuracy: 0.6306 - val_loss: 0.6526 - val_accuracy: 0.6285
Epoch 14/20
18/18 [==============================] - 63s 4s/step - loss: 0.6558 - accuracy: 0.6270 - val_loss: 0.6461 - val_accuracy: 0.6250
Epoch 15/20
18/18 [==============================] - 57s 3s/step - loss: 0.6541 - accuracy: 0.6199 - val_loss: 0.6521 - val_accuracy: 0.6094
Epoch 16/20
18/18 [==============================] - 59s 3s/step - loss: 0.6491 - accuracy: 0.6163 - val_loss: 0.6383 - val_accuracy: 0.6476
Epoch 17/20
18/18 [==============================] - 58s 3s/step - loss: 0.6470 - accuracy: 0.6448 - val_loss: 0.6622 - val_accuracy: 0.5833
Epoch 18/20
18/18 [==============================] - 61s 3s/step - loss: 0.6507 - accuracy: 0.6199 - val_loss: 0.6424 - val_accuracy: 0.6024
Epoch 19/20
18/18 [==============================] - 63s 4s/step - loss: 0.6447 - accuracy: 0.6372 - val_loss: 0.6451 - val_accuracy: 0.6181
Epoch 20/20
18/18 [==============================] - 57s 3s/step - loss: 0.6350 - accuracy: 0.6377 - val_loss: 0.6341 - val_accuracy: 0.6493
